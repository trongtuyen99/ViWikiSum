keyword: BERT (mô hình ngôn ngữ)
url: https://vi.wikipedia.org/wiki/BERT_(m%C3%B4_h%C3%ACnh_ng%C3%B4n_ng%E1%BB%AF)
content:
Biểu diễn Thể hiện Mã hóa Hai chiều từ Transformer (tiếng Anh: Bidirectional Encoder Representations from Transformers hay viết tắt là BERT) là một kỹ thuật học máy dựa trên các transformer được dùng cho việc huấn luyện trước xử lý ngôn ngữ tự nhiên (NLP) được phát triển bởi Google. Jacob Devlin và cộng sự từ Google đã tạo ra và công bố BERT vào năm 2018. Tính đến năm 2019, Google đã tận dụng BERT để hiểu rõ hơn các tìm kiếm của người dùng. Mô hình BERT bằng tiếng Anh ban đầu đi kèm với hai dạng tổng quát được đào tạo trước:: (1) mô hình the BERTBASE, kiến trúc mạng thần kinh chứa 12-lớp, 768-lớp ẩn, 12-đầu, 110M tham số, và (2) mô hình BERTLARGE model, kiến trúc mạng thần kinh chứa 24-lớp, 1024-lớp ẩn, 16-đầu, 340M tham số. Cả hai đều được huấn luyện từ BooksCorpus với 800M từ, và một phiên bản của Wikipedia tiếng Anh với 2,500M từ. BERT có nguồn gốc từ các biểu diễn theo ngữ cảnh trước đào tạo trước bao gồm học trình tự bán giám sát (semi-supervised sequence learning), Generative Pre-Training, ELMo, and ULMFit. biểu diễn ngôn ngữ không giám sát và hai chiều sâu, được đào tạo trước chỉ sử dụng một kho ngữ liệu văn bản thuần túy.
