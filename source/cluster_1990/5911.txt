keyword: Học tăng cường
url: https://thanhvie.com/tim-hieu-ve-q-learning-trong-hoc-tang-cuong/
content:
Một trong những thuật toán yêu thích mà tôi đã được học khi tham gia khóa học tăng cường (reinforcement learning) là q-learning. Lý do có lẽ là do nó khá dễ hiểu và có thể coding được. Bài viết này sẽ thảo luận về q-learning và thuật toán đằng sau nó. Q-learning là một thuật toán học-tăng-cường off policy ( off policy reinforcement learning algorithm - *** bài viết có khá nhiều thuật ngữ chuyên ngành, mình sẽ ưu tiên dùng tiếng Anh để giữ đúng ý tưởng nguyên bản của các thuật ngữ này, tránh bị hiểu sai đi khi dịch qua tiếng Việt. Chẳng hạn từ off policy có liên quan đến thuật toán tối ưu Markov, nếu dịch sang tiếng việt thì không có từ nào ngắn gọn mà có ý nghĩa tương đương cả). Nó được gọi là off policy vì thuật toán q learning học từ các hành động được thực hiện ngẫu nhiên, tức là không có trước một policy (*** policy hiểu theo thuật toán Markov thì có thể dịch là chính sách hoặc chiến lược) nào cả. Chữ Q đại diện cho chất lượng (Quality). Chất lượng trong trường hợp này là sự biểu thị của độ tốt của việc thực hiện một hành động (action) tới việc làm tăng phần thưởng trong dài hạn. Để thực hiện thuật toán q-learning, việc đầu tiên ta cần làm là tạo một bảng q (q - table). Q- table là một ma trận có chiều là (). Chúng ta khởi tạo giá trị 0 cho ma trận này. Sau đó, chúng ta cập nhật và lưu trữ các giá trị q vào q-table sau từng tập (episode - bao gồm tất cả các trạng thái được tính từ lúc tác nhân - agent (trong thuật toán Markov) bắt đầu học cho đến khi agent gặp điểm terminate). Bảng q này trở thành bảng tham chiếu để tác nhân (agent) chọn hành động tốt nhất (best action) dựa trên giá trị q (q value). Bước tiếp theo chỉ đơn giản là để tác nhân tương tác với môi trường và cập nhật các cặp hành động - trạng thái trong bảng q của chúng ta Q . Một tác nhân tương tác với môi trường (environment) theo 1 trong 2 cách. Đầu tiên là sử dụng q-table làm tham chiếu và xem tất cả các hành động có thể xảy ra cho một trạng thái nhất định. Sau đó, tác nhân sẽ chọn hành động dựa trên giá trị tối đa của những hành động đó. Đây được gọi là khai thác (exploit) vì chúng ta sử dụng thông tin có sẵn để đưa ra một quyết định (make a decision). Các cập nhật xảy ra sau mỗi bước hoặc hành động và kết thúc khi một tập (episode) được hoàn thành. Kết thúc trong trường hợp này có nghĩa là tác nhân (agent) đã tới được một số điểm cuối (terminate). Trạng thái cuối (terminate state) có thể là bất kỳ điều gì như truy cập trang thanh toán, đến cuối trò chơi, hoàn thành một số mục tiêu mong muốn, v.v. Tác nhân sẽ không học được nhiều chỉ sau một tập, nhưng với đủ sự khám phá (qua nhiều bước (steps) và tập (episode)) nó sẽ hội tụ và tìm được các giá trị q tối ưu hay còn gọi là q-sao (Q). Trong đoạn code ở trên, có một số biến mà chúng ta chưa đề cập đến. Cái chúng ta đang làm là điều chỉnh giá trị q dựa trên sự khác biệt giữa giá trị mới được chiết khấu (discounted new values) và giá trị cũ (old values). Chúng ta chiết khấu các giá trị mới bằng gamma và điều chỉnh kích thước bước bằng cách sử dụng tỷ lệ học (lr - learning rate). Dưới đây là một số giải thích. Tỉ lệ học (learning rate) lr: còn được gọi là alpha hay alpha, có thể đơn giản được định nghĩa là mức độ bạn chấp nhận giá trị mới so với giá trị cũ. Ở trên, chúng ta đang lấy sự khác biệt giữa mới và cũ và sau đó nhân giá trị đó với tỷ lệ học. Giá trị này sau đó được thêm vào giá trị q trước đó của chúng ta, về cơ bản sẽ đưa nó theo hướng cập nhật mới nhất. Gamma: gamma hoặc gamma là hệ số chiết khấu. Nó được sử dụng để cân bằng phần thưởng trước mắt và trong tương lai (hay trong dài hạn). Từ quy tắc cập nhật ở trên, có thể thấy rằng chúng ta đang áp dụng chiết khấu cho phần thưởng trong tương lai. Thông thường, giá trị này có thể nằm trong khoảng từ 0,8 đến 0,99. Phần thưởng: reward là giá trị nhận được sau khi hoàn thành một hành động nhất định tại một trạng thái nhất định. Phần thưởng có thể xảy ra ở bất kỳ bước nào (time steps) hoặc chỉ ở bước thời cuối cùng (terminate step). Max: np.max () sử dụng thư viện numpy để lấy giá trị cực đại của phần thưởng trong tương lai và áp dụng cho phần thưởng ở trạng thái hiện tại. Điều này tác động đến hành động hiện tại bằng phần thưởng có thể có trong tương lai. Đây là vẻ đẹp của q-learning. Chúng ta sẽ phân bổ phần thưởng tương lai cho các hành động hiện tại để giúp tác nhân chọn hành động mang lại lợi nhuận cao nhất ở bất kỳ trạng thái nhất định nào. Vậy là xong, ngắn gọn và (hy vọng) ngọt ngào. Chúng ta đã đi qua các quy tắc cập nhật cơ bản cho q-learning bằng cách sử dụng một số cú pháp python cơ bản và xem xét các đầu vào bắt buộc cho thuật toán. Chúng ta biết rằng q-learning sử dụng các phần thưởng trong tương lai để tác động đến hành động hiện tại với một trạng thái cho trước và do đó giúp tác nhân chọn các hành động tốt nhất để tối đa hóa tổng phần thưởng. Các bạn có thể tham khảo bài viết gốc tại đây. Học-tăng-cường (reinforcement learning) là một nhánh rất khó (và hot) của machine learning, nên bài viết sẽ không đặc biệt dễ hiểu (đặc biệt là các thuật ngữ chuyên ngành). Hy vọng bài viết có thể giúp ích được cho các bạn.
