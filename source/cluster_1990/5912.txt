keyword: Học tăng cường
url: https://ladigi.vn/q-learning-hoc-tang-cuong-la-gi-chi-tiet-ve-q-learning-hoc-tang-cuong-moi-nhat-2021
content:
Q -learning là một thuật toán học tăng cường không mô hình. Mục tiêu của Q-learning là học một chính sách, chính sách cho biết máy sẽ thực hiện hành động nào trong hoàn cảnh nào. Nó không yêu cầu một mô hình (do đó hàm ý “không mô hình”) của môi trường và nó có thể xử lý các vấn đề với chuyển đổi và phần thưởng ngẫu nhiên, mà không cần điều chỉnh.”. Đối với bất kỳ quá trình quyết định Markov hữu hạn (QTQDMKHH) nào, Q -learning tìm một chính sách tối ưu theo nghĩa là nó tối đa hóa giá trị mong đợi của tổng số phần thưởng trên bất kỳ và tất cả các bước tiếp theo, bắt đầu từ trạng thái hiện tại. Q -learning có thể xác định một chính sách lựa chọn hành động tối ưu cho bất kỳ QTQDMKHH cụ thể nào, với thời gian thăm dò vô hạn và chính sách một phần ngẫu nhiên. “Q” đặt tên theo tên hàm phần thưởng trả về, được sử dụng để cải thiện hoạt động và có thể nói là đại diện cho “chất lượng” của một hành động được thực hiện trong một trạng thái nhất định. Vào ngày hôm sau, do tình cờ ngẫu nhiên (thử nghiệm), bạn quyết định chờ đợi và để người khác xuống tàu trước. Điều này ban đầu dẫn đến thời gian chờ đợi lâu hơn. Tuy nhiên, thời gian chiến đấu với các hành khách khác là ít hơn. Nhìn chung, con đường này có phần thưởng cao hơn so với ngày hôm trước, vì tổng thời gian lên máy bay là:. Thông qua thăm dò, mặc dù hành động ban đầu (chờ đợi) dẫn đến chi phí (hoặc phần thưởng âm) lớn hơn so với chiến lược lao vào tàu, chi phí chung là thấp hơn, do đó là một chiến lược bổ ích hơn. Theo lý thuyết, việc lựa chọn hành động tiếp theo là ngẫu nhiên, nhưng trên thực tế, để đẩy nhanh tốc độ hội tụ, giải pháp tham lam được áp dụng, hành động có phần thưởng tiềm năng cao hơn sẽ được chọn và có 1 tỉ lệ ngẫu nhiên chọn các hành động khác. Bảng Q-Learning về các trạng thái được khởi tạo thành 0, sau đó mỗi ô được cập nhật thông qua đào tạo. Tỷ lệ học hoặc kích thước bước xác định mức độ thông tin mới thu được sẽ ghi đè thông tin cũ. Hệ số 0 làm cho tác nhân không học được gì (chỉ khai thác kiến thức trước), trong khi hệ số 1 khiến tác nhân chỉ xem xét thông tin gần đây nhất (bỏ qua kiến thức trước để khám phá các khả năng). Trong môi trường hoàn toàn xác định, tỷ lệ học tập của. Vì Q -learning là một thuật toán lặp, nó mặc nhiên thừa nhận một điều kiện ban đầu trước khi cập nhật đầu tiên xảy ra. Giá trị ban đầu cao, còn được gọi là “điều kiện ban đầu lạc quan”, có thể khuyến khích thăm dò: bất kể hành động nào được chọn, quy tắc cập nhật sẽ khiến nó có giá trị thấp hơn so với phương án khác, do đó tăng xác suất lựa chọn của chúng. Phần thưởng đầu tiên. Q -learning tại đơn giản nhất lưu trữ dữ liệu trong bảng. Cách tiếp cận này chùn bước với số lượng trạng thái / hành động ngày càng tăng. Q -learning có thể được kết hợp với xấp xỉ hàm. Điều này cho phép áp dụng thuật toán cho các vấn đề lớn hơn, ngay cả khi không gian trạng thái liên tục. Một giải pháp là sử dụng một mạng nơ ron nhân tạo (thích nghi) như một hàm xấp xỉ hàm. Xấp xỉ hàm có thể tăng tốc độ học tập trong các vấn đề hữu hạn, do thực tế là thuật toán có thể khái quát hóa các kinh nghiệm trước đó cho các trạng thái chưa từng thấy trước đây. Một kỹ thuật khác để giảm không gian trạng thái / hành động định lượng các giá trị có thể. Hãy xem xét ví dụ về việc học cách cân bằng một cây gậy trên ngón tay. Để mô tả trạng thái tại một thời điểm nhất định liên quan đến vị trí của ngón tay trong không gian, vận tốc của nó, góc của thanh và vận tốc góc của thanh. Điều này mang lại một vectơ bốn phần tử mô tả một trạng thái, tức là ảnh chụp nhanh của một trạng thái được mã hóa thành bốn giá trị. Vấn đề là vô số trạng thái có thể có mặt. Để thu hẹp không gian có thể của các hành động hợp lệ, nhiều giá trị có thể được gán cho một nhóm. Khoảng cách chính xác của ngón tay từ vị trí bắt đầu của nó (- vô cực đến + vô cực) không được biết, mà là nó có ở xa hay không (Gần, Xa). Q -learning được giới thiệu bởi Watkins vào năm 1989. Một bằng chứng hội tụ đã được trình bày bởi Watkins và Dayan vào năm 1992. Một bằng chứng toán học chi tiết hơn bởi Tsitsiklis vào năm 1994, và bởi Bertsekas và Tsitsiklis trong cuốn sách Lập trình động học Neuro năm 1996 của họ. Thuật ngữ “tăng cường thứ cấp”, được sử dụng từ lý thuyết học tập động vật, để mô hình hóa các giá trị trạng thái thông qua backpropagation: giá trị trạng thái v(s’) của tình huống hậu quả được đặt ngược lại cho các tình huống gặp phải trước đây. CAA tính toán các giá trị trạng thái theo chiều dọc và hành động theo chiều ngang (“thanh ngang”). Biểu đồ trình diễn cho thấy việc học tăng cường bị trì hoãn chứa các trạng thái (mong muốn, không mong muốn và trạng thái trung tính), được tính toán bởi hàm đánh giá trạng thái. Hệ thống học tập này là tiền thân của thuật toán Q-learning. Vào năm 2014, Google DeepMind đã cấp bằng sáng chế một ứng dụng Q-learning để học sâu, có tiêu đề “học tăng cường sâu” hoặc “Q-learning sâu” có thể chơi các trò chơi Atari 2600 ở cấp độ chuyên gia. Hệ thống DeepMind đã sử dụng một mạng lưới thần kinh tích chập sâu, với các lớp bộ lọc tích chập để mô phỏng các hiệu ứng của các lĩnh vực tiếp nhận. Học tăng cường không ổn định hoặc phân kỳ khi xấp xỉ hàm phi tuyến như mạng nơ ron được sử dụng để biểu diễn Q. Sự không ổn định này xuất phát từ các tương quan có trong chuỗi các quan sát, thực tế là các cập nhật nhỏ cho Q có thể thay đổi đáng kể chính sách và dữ liệu phân phối và mối tương quan giữa Q và các giá trị đích. Kỹ thuật sử dụng phát lại kinh nghiệm, một cơ chế lấy cảm hứng từ sinh học sử dụng một mẫu ngẫu nhiên các hành động trước thay vì hành động gần đây nhất để tiến hành. Điều này loại bỏ các mối tương quan trong chuỗi quan sát và làm mịn các thay đổi trong phân phối dữ liệu. Cập nhật lặp điều chỉnh Q theo các giá trị đích chỉ được cập nhật định kỳ, giảm hơn nữa tương quan với mục tiêu. Bởi vì giá trị hành động xấp xỉ tối đa trong tương lai trong Q-learning được đánh giá bằng cách sử dụng chức năng Q ở trong chính sách lựa chọn hành động hiện tại, trong môi trường nhiễu, Q-learning đôi khi có thể đánh giá quá cao các giá trị hành động, làm chậm quá trình học. Một biến thể được gọi là Double Q-learning đã được đề xuất để sửa lỗi này. Double Q-learning là một thuật toán học lại tăng cường ngoài chính sách, trong đó một chính sách khác được sử dụng để đánh giá giá trị so với chính sách được sử dụng để chọn hành động tiếp theo. Bây giờ giá trị ước tính của tương lai được đánh giá bằng cách sử dụng một chính sách khác, cái giải quyết vấn đề đánh giá quá cao. Thuật toán này sau đó được kết hợp với học sâu, thành thuật toán DQN, dẫn đến Double DQN, vượt trội hơn thuật toán DQN ban đầu. Q-learning bị trì hoãn là một triển khai thay thế của thuật toán Q -learning trực tuyến, với việc học đúng xấp xỉ với xác suất cao (PAC). GQ tham lam là một biến thể của Q -learning để sử dụng kết hợp với xấp xỉ hàm (tuyến tính). Ưu điểm của GQ tham lam là sự hội tụ được đảm bảo ngay cả khi sử dụng xấp xỉ hàm để ước tính các giá trị hành động. Học tăng cường: Giới thiệu của Richard Sutton và Andrew S. Barto, một cuốn sách giáo khoa trực tuyến. Củng cố học tập Mê cung, một cuộc biểu tình hướng dẫn một con kiến thông qua một mê cung sử dụng Q -learning. - Tận hưởng phim bản quyền Chất lượng cao độ phân giải 4K, FHD, âm thanh 5.1 và không quảng cáo như các web xem phim lậu. - Kho phim đồ sộ, các phim MỸ, TÂY BAN NHA, HÀN, TRUNG, NHẬT đều có đủ và 90% phim có Vietsub.
