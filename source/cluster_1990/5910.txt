keyword: Học tăng cường
url: https://thanhvie.com/hoc-tang-cuong-la-gi/
content:
Học tăng cường sâu (deep reinforcement learning) là sự kết hợp của học tăng cường (reinforcement learning) và học sâu (deep learning) và nó cũng là lĩnh vực học máy thịnh hành nhất tại thời điểm này bởi vì nó có thể giải quyết một loạt các nhiệm vụ ra quyết định phức tạp mà trước đây máy móc không có khả năng giải quyết các vấn đề trong thế giới thực giống như trí thông minh của con người. Hôm nay tôi bắt đầu một loạt bài về học tăng cường sâu để mang chủ đề này đến gần hơn với bạn đọc. Mục đích là để xem xét lĩnh vực này từ các thuật ngữ và biệt ngữ chuyên ngành đến các khái niệm cơ bản và thuật toán cổ điển, để giúp cho những người mới tham gia sẽ không bị lạc lối khi bắt đầu trong lĩnh vực tuyệt vời này. Tin tức thú vị về Trí tuệ nhân tạo (AI) liên tục xảy ra trong những năm gần đây. Ví dụ, AlphaGo đã đánh bại kỳ thủ chuyên nghiệp giỏi nhất của con người trong trò chơi cờ vây. Hoặc năm ngoái, chẳng hạn, người bạn Oriol Vinyals của chúng tôi và nhóm của anh ấy ở DeepMind đã cho thấy đặc vụ AlphaStar đánh bại những người chơi chuyên nghiệp trong trò chơi StarCraft II. Hoặc vài tháng sau, bot chơi Dota-2 của OpenAI đã trở thành hệ thống AI đầu tiên đánh bại các nhà vô địch thế giới trong một trò chơi thể thao điện tử. Tất cả các hệ thống này đều có điểm chung là chúng sử dụng Học tăng cường sâu (DRL). Nhưng trí tuệ nhân tạo (AI) và học tăng cường sâu (DRL) là gì?. Trí tuệ nhân tạo, lĩnh vực chính của khoa học máy tính, trong đó học tăng cường, là một ngành học liên quan đến việc tạo ra các chương trình máy tính hiển thị “trí thông minh” giống như con người. Học máy (ML) là một trong những cách tiếp cận phổ biến và thành công nhất đối với Trí tuệ nhân tạo, nhằm tạo ra các chương trình máy tính có thể tự động giải quyết các vấn đề bằng cách học từ dữ liệu. Học có giám sát (supervised learning) là nhiệm vụ học từ dữ liệu được gắn nhãn và mục tiêu của nó là tổng quát hóa (generalize). Học không giám sát (unsupervised learning) là nhiệm vụ học từ dữ liệu không được gắn nhãn và mục tiêu của nó là phân cụm (compress). Học tăng cường là nhiệm vụ học thông qua thử và sai và mục tiêu của nó là đưa ra quyết định. Trực giao với cách phân loại này, chúng ta có thể xem xét một cách tiếp cận mạnh mẽ gần đây đối với học máy, được gọi là Học sâu (Deep Learning). Học sâu không phải là một nhánh riêng biệt của học máy, vì vậy nó không phải là một nhiệm vụ học khác với những nhiệm vụ được mô tả ở trên. Học sâu là tập hợp các kỹ thuật và phương pháp sử dụng mạng nơ-ron để giải quyết các nhiệm vụ học máy, bao gồm học có giám sát, học không giám sát hoặc học tăng cường và chúng ta có thể biểu diễn nó bằng đồ thị trong hình sau:. Học sâu có thể giải quyết các vấn đề bằng cách sử dụng nhiều phương pháp và kỹ thuật học máy khác nhau, từ cây quyết định đến SVM, đến mạng nơ-ron. Tuy nhiên, trong loạt bài này, chúng ta chỉ sử dụng mạng nơ-ron; đây là phần "sâu" mà học tăng cường sâu đề cập đến. Tuy nhiên, mạng nơ-ron không hẳn là giải pháp tốt nhất cho mọi vấn đề. Ví dụ, mạng nơ-ron rất ngốn dữ liệu và khó diễn giải, nhưng phải nói rằng, mạng nơ-ron tại thời điểm này là một trong những kỹ thuật mạnh nhất hiện có và hiệu suất của chúng thường là tốt nhất. Học bằng cách tương tác với môi trường có lẽ là cách tiếp cận đầu tiên xuất hiện trong đầu chúng ta khi chúng ta nghĩ về bản chất của việc học. Đó là cách mà chúng ta học khi chúng ta còn là những đứa trẻ sơ sinh. Và chúng ta biết rằng những tương tác như vậy chắc chắn là nguồn kiến thức quan trọng về môi trường và bản thân trong suốt cuộc đời của mọi người, không chỉ với trẻ sơ sinh. Ví dụ, khi chúng ta học lái xe ô tô, chúng ta hoàn toàn nhận thức được cách môi trường phản ứng với những gì chúng ta làm, và chúng ta cũng tìm cách tác động đến những gì xảy ra trong môi trường thông qua hành động. Học từ sự tương tác là một khái niệm cơ bản làm nền tảng cho hầu hết các lý thuyết học tập và là nền tảng của học tăng cường. Cách tiếp cận của học tăng cường tập trung nhiều hơn vào việc học-hướng-tới-mục-tiêu từ sự tương tác hơn là các cách tiếp cận khác đối với học máy. Thực thể học tập (the learning entity) không được cho biết trước những hành động phải thực hiện, nhưng thay vào đó phải tự khám phá ra hành động nào tạo ra phần thưởng lớn nhất, hay chính là mục tiêu của nó, bằng cách kiểm tra các hành động này thông qua phương pháp "thử và sai". Hơn nữa, những hành động này không chỉ ảnh hưởng đến phần thưởng trước mắt mà còn ảnh hưởng đến phần thưởng trong tương lai, "phần thưởng bị trì hoãn", vì các hành động hiện tại sẽ quyết định các tình huống trong tương lai (như cách nó xảy ra trong cuộc sống thực). Hai đặc điểm này, "tìm kiếm thử và sai" và "phần thưởng bị trì hoãn", là hai đặc điểm đặc trưng của việc học tăng cường mà chúng ta sẽ đề cập trong suốt loạt bài đăng này. Học tăng cường là một lĩnh vực chịu ảnh hưởng của nhiều lĩnh vực khác trong việc giải quyết các vấn đề ra quyết định trong tình trạng không chắc chắn (decision - making problems under uncertainty). Ví dụ, lý thuyết điều khiển (control theory) nghiên cứu các cách điều khiển các hệ thống động lực học phức tạp, tuy nhiên động lực học của các hệ thống mà chúng ta cố gắng điều khiển thường được biết trước, không giống như trường hợp của học tăng cường sâu, không được biết trước. Một lĩnh vực khác là vận trù học (operations research) cũng nghiên cứu việc ra quyết định trong điều kiện không chắc chắn, nhưng thường xem xét các không gian hành động lớn hơn nhiều so với những không gian thường thấy trong học tăng cường. Kết quả là, có một sức mạnh tổng hợp giữa các lĩnh vực này, và điều này chắc chắn là tích cực cho sự tiến bộ của khoa học. Nhưng nó cũng mang lại một số mâu thuẫn trong thuật ngữ, ký hiệu, v.v. Đó là lý do tại sao trong phần này, chúng ta sẽ giới thiệu chi tiết về các thuật ngữ và ký hiệu mà chúng ta sẽ sử dụng trong loạt bài này. Tác nhân (agent), đại diện cho “giải pháp”, là một chương trình máy tính với một vai trò duy nhất là đưa ra quyết định để giải quyết các vấn đề ra quyết định phức tạp dưới sự không chắc chắn. Môi trường (environment), đó là đại diện của một “vấn đề”, là mọi thứ xảy ra sau quyết định của tác nhân. Ví dụ, trong trường hợp trò chơi ca rô, chúng ta có thể coi tác nhân là một trong những người chơi và môi trường bao gồm trò chơi trên bàn cờ và người chơi khác. Hai thành phần cốt lõi này tương tác liên tục theo cách mà tác nhân cố gắng tác động đến môi trường thông qua các hành động (hay quyết định) và môi trường phản ứng lại với các hành động của tác nhân. Cách môi trường phản ứng với các hành động nhất định được xác định bởi một mô hình mà tác nhân có thể biết hoặc có thể không biết và điều này phân biệt hai trường hợp:. Khi tác nhân biết mô hình, chúng ta gọi tình huống này là học tăng cường dựa trên mô hình (model-based reinforcement learning). Trong trường hợp này, khi chúng ta hiểu biết đầy đủ về môi trường, chúng ta có thể tìm ra giải pháp tối ưu bằng quy hoạch động (dynamic programming). Trường hợp này không phải là mục đích của bài viết. Khi tác nhân không biết mô hình, nó cần đưa ra quyết định với thông tin không đầy đủ; đây là việc học tăng cường không có mô hình (model-free reinforcement learning) hoặc cố gắng tìm hiểu mô hình một cách từ từ như một phần của thuật toán. Môi trường được biểu diễn bằng một tập hợp các biến (variables) liên quan đến vấn đề (các biến này hoàn toàn phụ thuộc vào loại vấn đề mà chúng ta muốn giải quyết).Tập hợp các biến này và tất cả các giá trị mà chúng có thể nhận được gọi là không gian trạng thái (state space).Trạng thái (state) là một khởi tạo của không gian trạng thái, một tập hợp các giá trị mà các biến nhận lấy. Trong học tăng cường thì các trạng thái còn được gọi là các quan sát (vì tác nhân trong thực tế không biết đầy đủ các trạng thái của môi trường mà chỉ biết được một phần các trạng thái đó - các trạng thái không đầy đủ mà tác nhân biết thì được gọi là các quan sát). Tại mỗi trạng thái, môi trường tạo sẵn một tập hợp các hành động, từ đó tác nhân sẽ chọn một hành động. Tác nhân ảnh hưởng đến môi trường thông qua các hành động này và môi trường có thể thay đổi trạng thái như một phản ứng đối với hành động do tác nhân thực hiện. Hàm chịu trách nhiệm về sự tương tác này được gọi là hàm chuyển tiếp (transition function) hoặc xác suất chuyển đổi (transition probabilities) giữa các trạng thái. Môi trường thường chứa đựng một nhiệm vụ được xác định rõ ràng và có thể cung cấp cho tác nhân một tín hiệu khen thưởng như một câu trả lời trực tiếp cho các hành động của tác nhân. Phần thưởng này là phản hồi về mức độ hiệu quả của hành động cuối cùng của tác nhân trong việc nó góp phần đạt được nhiệm vụ. Phần thưởng này được thực hiện bởi môi trường. Hàm chịu trách nhiệm về ánh xạ này được gọi là hàm phần thưởng (reward function) hoặc xác suất phần thưởng (reward probabilities). Như chúng ta sẽ thấy ở phần sau, mục tiêu của tác nhân là tối đa hóa phần thưởng tổng thể mà nó nhận được và do đó, phần thưởng là động lực mà tác nhân cần để thực hiện hành vi mong muốn. Hình vẽ bên dưới sẽ tóm tắt các đặc tính trong học tăng cường vừa được giới thiệu. Chu kỳ bắt đầu với việc tác nhân quan sát môi trường (bước 1) và nhận về một trạng thái và một phần thưởng. Tác nhân sử dụng trạng thái và phần thưởng này để quyết định hành động tiếp theo cần thực hiện (bước 2). Sau đó, tác nhân sẽ gửi một hành động tới môi trường nhằm cố gắng kiểm soát nó theo cách có lợi (bước 3). Cuối cùng, môi trường chuyển đổi và trạng thái bên trong của nó thay đổi do hệ quả của trạng thái và hành động trước đó của tác nhân (bước 4). Sau đó, chu kỳ lặp lại. Nhiệm vụ mà tác nhân đang cố gắng giải quyết có thể có hoặc không có kết thúc tự nhiên. Các nhiệm vụ có kết thúc tự nhiên, chẳng hạn như một trò chơi, được gọi là các nhiệm vụ nhiều tập (episodic tasks). Ngược lại, các nhiệm vụ sẽ được gọi là nhiệm vụ liên tục (continuing tasks), chẳng hạn như học chuyển động về phía trước (ví dụ như game Flappy Birds). Trình tự các bước thời gian (time steps) từ đầu đến cuối của một nhiệm vụ nhiều tập được gọi là một tập (episode). Như chúng ta sẽ thấy, tác nhân có thể thực hiện một số bước thời gian trong một tập và thực hiện nhiều tập để tìm hiểu cách giải quyết một nhiệm vụ. Tổng phần thưởng thu được trong một tập được gọi là lợi nhuận hay lợi tức (return). Các tác nhân thường được thiết kế để tối đa hóa lợi nhuận này. Một trong những hạn chế là những phần thưởng này không được tiết lộ cho tác nhân cho đến khi kết thúc một tập, do đó gọi là "phần thưởng bị trì hoãn". Ví dụ, trong trò chơi ca rô, phần thưởng cho mỗi chuyển động (hành động) riêng lẻ sẽ không được biết cho đến khi kết thúc trò chơi. Đó sẽ là phần thưởng dương nếu tác nhân thắng trò chơi (vì tác nhân đã đạt được kết quả mong muốn) hoặc phần thưởng âm (hình phạt) nếu tác nhân thua trò chơi. Một đặc điểm quan trọng khác và là thách thức trong học tăng cường, là sự đánh đổi giữa “thăm dò” (explore) và “khai thác” (exploit). Cố gắng đạt được nhiều phần thưởng, tác nhân phải ưu tiên những hành động mà nó đã thử trong quá khứ và biết rằng đó sẽ là những hành động hiệu quả trong việc tạo ra phần thưởng. Nhưng nghịch lý ở chỗ là để phát hiện ra những hành động như vậy, nó phải thử những hành động mà nó chưa chọn bao giờ. Tóm lại, tác nhân phải khai thác những gì đã trải qua để nhận được nhiều phần thưởng nhất có thể, nhưng đồng thời, tác nhân cũng phải khám phá để lựa chọn hành động tốt hơn trong tương lai. Thế tiến thoái lưỡng nan về thăm dò-khai thác là một chủ đề quan trọng và vẫn là một chủ đề nghiên cứu chưa được giải đáp. Chúng ta sẽ nói về sự đánh đổi này sau. Trong phần này, tôi sẽ giới thiệu Frozen-Lake, một môi trường thế-giới-lưới (grid-world) đơn giản từ Gym, một bộ công cụ để phát triển và so sánh các thuật toán học tăng cường. Với ví dụ này về môi trường, chúng ta sẽ xem xét và làm rõ thuật ngữ học tăng cường được giới thiệu cho đến bây giờ, và nó cũng sẽ hữu ích cho các bài viết sau. Môi trường Frozen-Lake thuộc danh mục được gọi là thế - giới - lưới (grid-world), khi tác nhân sống trong một lưới kích thước 4x4 (có 16 ô), có nghĩa là một không gian trạng thái bao gồm 16 trạng thái (0–15) dựa trên toạ độ i, j của thế - giới - lưới. Trong Frozen-Lake, tác nhân luôn bắt đầu ở vị trí trên cùng bên trái và mục tiêu của nó là đến vị trí dưới cùng bên phải của lưới. Có bốn lỗ trong các ô cố định của lưới và nếu tác nhân lọt vào các lỗ đó, một tập sẽ kết thúc và phần thưởng nhận được bằng 0. Nếu tác nhân đến ô đích, thì nó sẽ nhận được phần thưởng là 1 và một tập kết thúc. Hình sau đây mô tả trực quan về môi trường hồ đóng băng:. Để đạt được mục tiêu, tác nhân có một không gian hành động (action space) bao gồm bốn hướng chuyển động: lên, xuống, trái và phải. Chúng ta cũng biết rằng có một hàng rào xung quanh hồ, vì vậy nếu tác nhân cố gắng di chuyển ra khỏi thế - giới - lưới, nó sẽ chỉ quay trở lại ô mà nó đã cố gắng di chuyển. Bởi vì hồ đóng băng có thuộc tính trơn trượt, do đó, hành động của tác nhân không phải lúc nào cũng diễn ra như mong đợi - có 33% khả năng nó sẽ trượt sang phải hoặc sang trái. Ví dụ: nếu chúng ta muốn tác nhân di chuyển sang trái, có 33% xác suất rằng nó thực sự sẽ di chuyển sang trái, 33% khả năng nó sẽ kết thúc ở ô phía trên và 33% khả năng nó sẽ kết thúc ở ô bên dưới. Hành vi này của môi trường được phản ánh trong hàm chuyển đổi hoặc xác suất chuyển đổi đã được trình bày trước đó. Tuy nhiên, tại thời điểm này chúng ta không cần đi sâu hơn vào chức năng này mà để sau.
