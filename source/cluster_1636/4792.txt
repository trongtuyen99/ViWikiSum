keyword: Hàm kích hoạt
url: https://qastack.vn/ai/5493/what-is-the-purpose-of-an-activation-function-in-neural-networks
content:
Người ta nói rằng các chức năng kích hoạt trong các mạng thần kinh giúp giới thiệu phi tuyến tính . Hầu như tất cả các chức năng được cung cấp bởi các chức năng kích hoạt phi tuyến tính được đưa ra bởi các câu trả lời khác. Hãy để tôi tổng hợp chúng:. Tại sao nó giúp? Tôi hầu như không nghĩ rằng bạn có thể tìm thấy bất kỳ hiện tượng thế giới vật lý nào theo tuyến tính một cách đơn giản. Vì vậy, bạn cần một hàm phi tuyến tính có thể xấp xỉ hiện tượng phi tuyến tính. Ngoài ra, một trực giác tốt sẽ là bất kỳ ranh giới quyết định hoặc hàm nào là sự kết hợp tuyến tính của các kết hợp đa thức của các tính năng đầu vào (vì vậy cuối cùng là phi tuyến tính). Mục đích của chức năng kích hoạt? Ngoài việc giới thiệu phi tuyến tính, mọi chức năng kích hoạt đều có các tính năng riêng. Đây là một trong những chức năng kích hoạt phổ biến nhất và đang tăng đơn điệu ở mọi nơi. Điều này thường được sử dụng tại nút đầu ra cuối cùng vì nó nén các giá trị trong khoảng từ 0 đến 1 (nếu đầu ra là bắt buộc 0hoặc 1). Vì vậy, trên 0,5 được xem xét 1trong khi dưới 0,5 như 0, mặc dù 0.5có thể đặt ngưỡng khác (không ). Ưu điểm chính của nó là sự khác biệt của nó rất dễ dàng và sử dụng các giá trị đã được tính toán và các tế bào thần kinh cua được cho là có chức năng kích hoạt này trong các tế bào thần kinh của chúng. Điều này có lợi thế hơn chức năng kích hoạt sigmoid vì nó có xu hướng tập trung đầu ra thành 0, có tác dụng học tập tốt hơn trên các lớp tiếp theo (hoạt động như một trình chuẩn hóa tính năng). Một lời giải thích tốt đẹp ở đây . Giá trị đầu ra tiêu cực và tích cực có thể được coi là 0và 1tương ứng. Được sử dụng chủ yếu trong RNNs. Hàm kích hoạt Re-Lu - Đây là một hàm kích hoạt phi tuyến tính đơn giản (tuyến tính trong phạm vi dương và phạm vi phủ định khác nhau) có ưu điểm là loại bỏ vấn đề biến mất độ dốc đối mặt với hai độ dốc trên có nghĩa là0như x có xu hướng + vô cực hoặc vô cực. Dưới đây là câu trả lời về sức mạnh gần đúng của Re-Lu mặc dù tính tuyến tính rõ ràng của nó. ReLu có nhược điểm là có các nơ-ron chết, dẫn đến NN lớn hơn. Ngoài ra, bạn có thể thiết kế các chức năng kích hoạt của riêng bạn tùy thuộc vào vấn đề chuyên ngành của bạn. Bạn có thể có một hàm kích hoạt bậc hai sẽ xấp xỉ các hàm bậc hai tốt hơn nhiều. Nhưng sau đó, bạn phải thiết kế một hàm chi phí có phần hơi lồi trong tự nhiên, để bạn có thể tối ưu hóa nó bằng cách sử dụng các khác biệt thứ tự đầu tiên và NN thực sự hội tụ đến một kết quả tốt. Đây là lý do chính tại sao các chức năng kích hoạt tiêu chuẩn được sử dụng. Nhưng tôi tin rằng với các công cụ toán học thích hợp, có một tiềm năng rất lớn cho các hàm kích hoạt mới và lập dị. Vì vậy, chúng tôi nhận được một sự kết hợp rất phức tạp, với tất cả các kết hợp đa thức có thể có của các biến đầu vào hiện tại. Tôi tin rằng nếu Mạng thần kinh được cấu trúc chính xác, NN có thể tinh chỉnh các kết hợp đa thức này bằng cách sửa đổi các trọng số kết nối và chọn các thuật ngữ đa thức tối đa hữu ích và loại bỏ các thuật ngữ bằng cách trừ đi đầu ra của 2 nút có trọng số. Nhưng đối với một bằng chứng toán học chính thức, người ta phải xem Định lý xấp xỉ phổ quát. Nếu bạn chỉ có các lớp tuyến tính trong một mạng nơ-ron, thì tất cả các lớp về cơ bản sẽ sụp đổ thành một lớp tuyến tính, và do đó, kiến trúc mạng nơ-ron "sâu" thực sự sẽ không còn sâu nữa mà chỉ là một phân loại tuyến tính. Trong đó WWW tương ứng với ma trận đại diện cho trọng số và độ lệch của mạng cho một lớp và f()f()f() cho hàm kích hoạt. Bây giờ, với sự ra đời của một đơn vị kích hoạt phi tuyến tính sau mỗi lần chuyển đổi tuyến tính, điều này sẽ không xảy ra nữa. Bây giờ mỗi lớp có thể dựa trên kết quả của lớp phi tuyến trước đó, về cơ bản dẫn đến một hàm phi tuyến tính phức tạp, có thể xấp xỉ mọi hàm có thể với trọng số phù hợp và đủ độ sâu / chiều rộng. Trước tiên hãy nói về tuyến tính . Tuyến tính có nghĩa là bản đồ (một hàm),f: V→ Wf:V→Wf: V rightarrow W, được sử dụng là một bản đồ tuyến tính, nghĩa là nó thỏa mãn hai điều kiện sau. Bạn nên làm quen với định nghĩa này nếu bạn đã nghiên cứu đại số tuyến tính trong quá khứ. Tuy nhiên, điều quan trọng hơn là phải nghĩ đến tính tuyến tính về khả năng phân tách tuyến tính của dữ liệu, có nghĩa là dữ liệu có thể được tách thành các lớp khác nhau bằng cách vẽ một đường (hoặc siêu phẳng, nếu nhiều hơn hai chiều), đại diện cho ranh giới quyết định tuyến tính, thông qua dữ liệu. Nếu chúng ta không thể làm điều đó, thì dữ liệu không thể phân tách tuyến tính. Thông thường, dữ liệu từ một thiết lập vấn đề phức tạp hơn (và do đó phù hợp hơn) không thể phân tách tuyến tính, do đó, chúng ta quan tâm đến việc mô hình hóa chúng. Để mô hình hóa ranh giới quyết định phi tuyến của dữ liệu, chúng ta có thể sử dụng mạng thần kinh giới thiệu tính phi tuyến tính. Các mạng thần kinh phân loại dữ liệu không thể phân tách tuyến tính bằng cách chuyển đổi dữ liệu bằng cách sử dụng một số hàm phi tuyến (hoặc chức năng kích hoạt của chúng tôi), do đó các điểm được chuyển đổi kết quả trở nên tách rời tuyến tính. Các chức năng kích hoạt khác nhau được sử dụng cho các bối cảnh thiết lập vấn đề khác nhau. Bạn có thể đọc thêm về điều đó trong cuốn sách Deep Learning (loạt tính toán thích ứng và học máy) . Để biết ví dụ về dữ liệu không phân tách tuyến tính, hãy xem tập dữ liệu XOR. Hãy xem xét một mạng lưới thần kinh rất đơn giản, chỉ có 2 lớp, trong đó lớp đầu tiên có 2 nơ-ron và 1 nơ-ron cuối cùng và kích thước đầu vào là 2. Chúng tôi không có kích hoạt, vì vậy đầu ra của các tế bào thần kinh trong lớp đầu tiên là
