keyword: Hàm kích hoạt
url: https://aicurious.io/posts/2019-09-23-cac-ham-kich-hoat-activation-function-trong-neural-networks/
content:
Hàm kích hoạt (activation function) mô phỏng tỷ lệ truyền xung qua axon của một neuron thần kinh. Trong một mạng nơ-ron nhân tạo, hàm kích hoạt đóng vai trò là thành phần phi tuyến tại output của các nơ-ron. Trong bài viết này, chúng ta sẽ cùng tìm hiểu các hàm kích hoạt phổ biến nhất và các ưu, nhược điểm của chúng. Câu trả lời là nếu không có các hàm kích hoạt phi tuyến, thì mạng nơ-ron của chúng ta dù có nhiều lớp vẫn sẽ có hiệu quả như một lớp tuyến tính mà thôi. Để hiểu rõ hơn và trực quan hơn, ta sẽ xem xét một mạng nơ-ron nhỏ gồm 2 lớp (có các hàm kích hoạt tuyến tính tương ứng). Mạng này sẽ nhận vào input là X và trả ra output Y:. Trong lớp thứ nhất, ta có trọng số W^ , hệ số bias B^. Output Z^ được tính như sau:. Vì ta đang giả sử hàm kích hoạt g() là tuyến tính nên a^ = c*Z^ (c là một số thực). Để cho đơn giản, ta giả sử c = 1. Sử dụng các công thức bên trên, ta có:. a^ thực chất vẫn chỉ là một hàm tuyến tính của X ban đầu. Vì thế, việc xếp chồng các lớp nơ-ron lên nhau là vô nghĩa. Vậy trên thực tế có bao giờ dùng các hàm kích hoạt tuyến tính không? Vẫn có một số trường hợp các hàm kích hoạt tuyến tính được sử dụng. Ví dụ trong bài toán regression, kết quả đầu ra Y là một số thực, hàm kích hoạt ngay phía trước Y có thể là một hàm tuyến tính. Dù thế, các hàm kích hoạt ở các lớp ẩn (hidden layer) bắt buộc phải có các yếu tố phi tuyến. Ở các phần dưới đây, tôi xin giới thiệu đến các bạn các hàm kích hoạt thường dùng kèm theo giải thích về ưu và nhược điểm của chúng. Hàm Sigmoid nhận đầu vào là một số thực và chuyển thành một giá trị trong khoảng (0;1) (xem đồ thị phía trên). Đầu vào là số thực âm rất nhỏ sẽ cho đầu ra tiệm cận với 0, ngược lại, nếu đầu vào là một số thực dương lớn sẽ cho đầu ra là một số tiệm cận với 1. Trong quá khứ hàm Sigmoid hay được dùng vì có đạo hàm rất đẹp. Tuy nhiên hiện nay hàm Sigmoid rất ít được dùng vì những nhược điểm sau:. Hàm Sigmoid bão hào và triệt tiêu gradient: Một nhược điểm dễ nhận thấy là khi đầu vào có trị tuyệt đối lớn (rất âm hoặc rất dương), gradient của hàm số này sẽ rất gần với 0. Điều này đồng nghĩa với việc các hệ số tương ứng với unit đang xét sẽ gần như không được cập nhật (còn được gọi là vanishing gradient). Để trực quan hơn, hãy xét trường hợp có 2 trọng số w1 và w2 cần được tối ưu. Khi gradient luôn có cùng dấu, chúng ta sẽ chỉ có thể di chuyển hướng đông bắc hoặc hướng tây nam trong đồ thị phía dưới để đến được điểm tối ưu. Nếu điểm tối ưu nằm ở phía đông nam như hình dưới, chúng ta sẽ cần đi đường zig zag để đến đó. Rất may chúng ta có thể giải quyết vấn đề này bằng cách chuẩn hoá dữ liệu về dạng có trung tâm là 0 (zero-centered) với các thuật toán batch/layer normalization. Hàm tanh nhận đầu vào là một số thực và chuyển thành một giá trị trong khoảng (-1; 1). Cũng như Sigmoid, hàm Tanh bị bão hoà ở 2 đầu (gradient thay đổi rất ít ở 2 đầu). Tuy nhiên hàm Tanh lại đối xứng qua 0 nên khắc phục được một nhược điểm của Sigmoid. Hàm ReLU đang được sử dụng khá nhiều trong những năm gần đây khi huấn luyện các mạng neuron. ReLU đơn giản lọc các giá trị < 0. Nhìn vào công thức chúng ta dễ dàng hiểu được cách hoạt động của nó. Một số ưu điểm khá vượt trội của nó so với Sigmoid và Tanh:. (+) Tốc độ hội tụ nhanh hơn hẳn. ReLU có tốc độ hội tụ nhanh gấp 6 lần Tanh (Krizhevsky et al.). Điều này có thể do ReLU không bị bão hoà ở 2 đầu như Sigmoid và Tanh. (+) Tính toán nhanh hơn. Tanh và Sigmoid sử dụng hàm exp và công thức phức tạp hơn ReLU rất nhiều do vậy sẽ tốn nhiều chi phí hơn để tính toán. (-) Tuy nhiên ReLU cũng có một nhược điểm: Với các node có giá trị nhỏ hơn 0, qua ReLU activation sẽ thành 0, hiện tượng đấy gọi là “Dying ReLU“. Nếu các node bị chuyển thành 0 thì sẽ không có ý nghĩa với bước linear activation ở lớp tiếp theo và các hệ số tương ứng từ node đấy cũng không được cập nhật với gradient descent. (-) Khi learning rate lớn, các trọng số (weights) có thể thay đổi theo cách làm tất cả neuron dừng việc cập nhật. f(x) = mathbb(x < 0) (alpha x) + mathbb(x>=0) (x) với alpha là hằng số nhỏ. Thay vì trả về giá trị 0 với các đầu vào <0 thì Leaky ReLU tạo ra một đường xiên có độ dốc nhỏ (xem đồ thị). Có nhiều báo cáo về việc hiệu Leaky ReLU có hiệu quả tốt hơn ReLU, nhưng hiệu quả này vẫn chưa rõ ràng và nhất quán. Ngoài Leaky ReLU có một biến thể cũng khá nổi tiếng của ReLU là PReLU. PReLU tương tự Leaky ReLU nhưng cho phép neuron tự động chọn hệ số alpha tốt nhất. Khi đến với Maxout, chúng ta sẽ không sử dụng công thức dạng f(w^Tx + b) nữa. Một dạng khá phổ biến là Maxout neuron (giới thiệu bởi Goodfellow et al.)) Tuy vậy, nó khiến mạng phải sử dụng gấp đôi số tham số (parameter) cho mỗi neuron, vì thế làm tăng đáng kể chi phí cả về bộ nhớ và tính toán - một điều cần suy xét khi huấn luyện mạng deep learning ở hiện tại. Câu trả lời là tuỳ bài toán. Khi các bạn tìm hiểu về các cấu trúc mạng cụ thể, các activation khác nhau sẽ được sử dụng, tuỳ vào độ sâu của mạng, output mong muốn, thậm chí là dữ liệu của bài toán. Chúng ta không thể nói hàm nào tốt hơn khi chưa xét đên điều kiện cụ thể. Để bắt đầu, khi các bạn học về cấu trúc mạng nào, hãy quan sát thiết kế và mã nguồn của mạng được viết bởi phần đông mọi người. Việc này thường cho kết quả tương đối tốt.
