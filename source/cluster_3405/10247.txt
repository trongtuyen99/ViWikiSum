keyword: Điểm kỳ dị công nghệ
url: https://qastack.vn/ai/17/what-is-the-concept-of-the-technological-singularity
content:
Điểm kỳ dị về công nghệ là một điểm lý thuyết trong thời gian mà trí thông minh nhân tạo tự cải thiện có thể hiểu và điều khiển các khái niệm bên ngoài phạm vi của bộ não con người, đó là thời điểm mà nó có thể hiểu được con người, bằng thiết kế sinh học, có thể t. Sự mờ nhạt về điểm kỳ dị xuất phát từ thực tế là, từ điểm kỳ dị trở đi, lịch sử thực sự khó lường. Loài người sẽ không thể dự đoán bất kỳ sự kiện nào trong tương lai, hoặc giải thích bất kỳ sự kiện hiện tại nào, vì chính khoa học trở nên không có khả năng mô tả các sự kiện kích hoạt bằng máy. Về cơ bản, máy móc sẽ nghĩ về chúng ta giống như cách chúng ta nghĩ về loài kiến. Vì vậy, chúng ta có thể không đưa ra dự đoán nào qua điểm kỳ dị. Hơn nữa, như một hệ quả logic, chúng ta không thể xác định điểm mà tại đó điểm kỳ dị có thể xảy ra, hoặc thậm chí nhận ra nó khi nó xảy ra. Tuy nhiên, để sự kỳ dị diễn ra, AGI cần được phát triển, và liệu điều đó có khả thi hay không là một cuộc tranh luận sôi nổingay bây giờ. Hơn nữa, một thuật toán tạo ra trí thông minh siêu phàm từ các bit và byte sẽ phải được thiết kế. Theo định nghĩa, một lập trình viên con người sẽ không thể làm được điều đó, vì bộ não của anh ta / cô ta sẽ cần có khả năng hiểu các khái niệm vượt ra ngoài phạm vi của nó. Ngoài ra còn có lập luận rằng một vụ nổ trí thông minh (cơ chế mà một điểm kỳ dị công nghệ sẽ được hình thành về mặt lý thuyết) sẽ không thể xảy ra do khó khăn trong thách thức thiết kế khiến nó trở nên thông minh hơn, tỷ lệ thuận với trí thông minh của nó và khó khăn hơn bản thân thiết kế có thể vượt qua trí thông minh cần thiết để giải quyết thách thức nói trên (điểm cuối cùng cho thần llamas trong các bình luận). Ngoài ra, có những lý thuyết liên quan liên quan đến máy móc chiếm lĩnh loài người và tất cả những câu chuyện khoa học viễn tưởng đó. Tuy nhiên, điều đó khó có thể xảy ra, nếu luật pháp của Asimov được tuân thủ một cách thích hợp. Ngay cả khi luật pháp của Asimov chưa đủ, một loạt các ràng buộc vẫn sẽ là cần thiết để tránh sự lạm dụng AGI của các cá nhân bị hiểu lầm và luật pháp của Asimov là điều gần nhất chúng ta phải làm. Khái niệm "điểm kỳ dị" là khi máy móc vượt qua con người. Mặc dù ý kiến của Stephen Hawking là tình huống này là không thể tránh khỏi, nhưng tôi nghĩ sẽ rất khó đạt được điểm đó, bởi vì mọi thuật toán AI cần được lập trình bởi con người, do đó nó sẽ luôn bị hạn chế hơn so với người tạo ra nó. Có lẽ chúng ta sẽ biết khi nào nhân loại sẽ mất quyền kiểm soát Trí tuệ nhân tạo, nơi AI siêu thông minh sẽ cạnh tranh với con người và có thể tạo ra những sinh vật thông minh tinh vi hơn xảy ra, nhưng hiện tại, nó giống như khoa học viễn tưởng (hay còn gọi là Skynet của Terminator ). Nguy cơ có thể liên quan đến giết người (như tự bay chiến UAV đưa ra quyết định của riêng mình), các nước phá hủy hoặc thậm chí toàn bộ hành tinh (như AI kết nối với vũ khí hạt nhân (aka WarGames phim), nhưng nó không chứng minh quan điểm rằng máy móc sẽ thông minh hơn con người. Điểm kỳ dị, trong bối cảnh của AI, là một sự kiện lý thuyết, theo đó một hệ thống thông minh với các tiêu chí sau được triển khai. Bằng cách cảm ứng, lý thuyết sau đó dự đoán rằng một chuỗi các sự kiện sẽ được tạo ra với tốc độ tăng trí thông minh tiềm năng có thể vượt quá tốc độ tiến hóa của não. Làm thế nào bắt buộc thực thể tự cải thiện này hoặc dân số của các thực thể được điều trị sẽ là gì để bảo vệ cuộc sống và tự do của con người là không xác định. Ý tưởng rằng một nghĩa vụ như vậy có thể là một phần của hợp đồng phần mềm không thể hủy bỏ là ngây thơ trong bản chất của các khả năng gắn liền với các tiêu chí (1) đến (4) ở trên. Cũng như các công nghệ mạnh mẽ khác, các rủi ro cũng nhiều và sâu rộng như các lợi ích tiềm năng. Rủi ro cho nhân loại không đòi hỏi trí thông minh. Có những bối cảnh khác để sử dụng thuật ngữ số ít, nhưng chúng nằm ngoài phạm vi của diễn đàn AI này nhưng có thể đáng được đề cập ngắn gọn cho rõ ràng. Kỹ thuật di truyền, kỹ thuật hạt nhân, toàn cầu hóa và dựa trên nền kinh tế quốc tế dựa trên nguồn năng lượng hữu hạn được tiêu thụ nhanh hơn hàng ngàn lần so với nó phát sinh trên trái đất - Đây là những ví dụ khác về công nghệ rủi ro cao và xu hướng đại chúng gây ra rủi ro cũng như lợi ích đến nhân loại. Quay trở lại với AI, sự cảnh báo chính trong lý thuyết về điểm kỳ dị là sự thất bại trong việc kết hợp xác suất. Mặc dù có thể phát triển một thực thể phù hợp với các tiêu chí (1) đến (4) ở trên, nhưng nó có thể không đủ khả năng để sự kiện đầu tiên xảy ra lâu sau khi tất cả các ngôn ngữ hiện tại trên Trái đất đã chết. Ở một thái cực khác của phân phối xác suất, người ta có thể dễ dàng lập luận rằng có một xác suất khác không rằng sự kiện đầu tiên đã xảy ra. Dọc theo những dòng đó, nếu một sự hiện diện thông minh hơn nơi đã tồn tại trên Internet, thì có khả năng nó sẽ tìm thấy nó trong lợi ích tốt nhất của mình để tiết lộ cho những người thấp hơn. Chúng ta có tự giới thiệu về một con giòi đi qua không?. "Điểm kỳ dị", được xem hẹp, đề cập đến một điểm mà tốc độ tăng trưởng kinh tế nhanh đến mức chúng ta không thể đưa ra dự đoán hữu ích về quá khứ tương lai sẽ như thế nào. Nó thường được sử dụng thay thế cho "vụ nổ trí thông minh", đó là khi chúng ta có cái gọi là AI mạnh, đó là AI đủ thông minh để hiểu và cải thiện chính nó. Có vẻ hợp lý khi hy vọng rằng vụ nổ tình báo sẽ ngay lập tức dẫn đến một điểm kỳ dị về kinh tế, nhưng điều ngược lại không nhất thiết là đúng.
